# sathya-nmt

DATA PREPARATION (data_prep.sh) -> spmmodel-create.py (SENTENCE PIECE MODEL CREATION) # Delete the .vocab generated by Sentence Piece at this stage -> BUILD VOCABULARY ->tokenizaitonoftestset (bashscript) -> TRAINING ->  TRANSLATE -> DETOKENIZATION using spm_decode -> BLEUSCORE using sacrebleu.

Python_3.6.9 is used.

`python -m venv environment`

`source environment/bin/activate`

`pip install --upgrade pip`

`pip install torch==1.6.0`

`pip install sacrebleu`


`git clone https://github.com/OpenNMT/OpenNMT-py.git`

`cd OpenNMT-py`

`python setup.py install`

# Now edit requirements: git -> https:

`pip install -r requirements.opt.txt`
--------------------------------------------

`mkdir data data/models`

`bash data_prep.sh`

`python spmmodel-create.py hi`

`python build_vocab.py -config config_vocab.yml`

This will generate vocabulary file.

`bash tokenizationoftestingset.sh`

`python main.py -config config_transformer.yml`  #### RNN or TRANSFORMER

`python inference.py -config config_translate.yml` 

After inference convert the tokenised output to text using same BPE model as used for encoding.

# Install Sentence Piece as well before running 

`spm_decode -model=data/models/spm_model.bp.en.hi.model -input_format=piece < data/en-hi/testoutputspm.en > data/en-hi/testoutput.en` 

`sacrebleu data/en-hi/test.en < data/en-hi/testoutput.en`

